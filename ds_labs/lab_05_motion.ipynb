{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a86c853b-b82f-4883-bf03-15a2c05b87db",
   "metadata": {},
   "source": [
    "# Week 8: Motion Detection\n",
    "\n",
    "<font size=\"6\"> Laboratory 5 </font> <br>\n",
    "<font size=\"3\"> Last updated July 25, 2023 </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e90879f5-f730-44da-ad7a-be77ee8b2eaa",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange;\"> 00. Content </span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba827722-0c2f-493b-a2d2-d7f0bda08882",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Mathematics </font>\n",
    "- N/A\n",
    "     \n",
    "<font size=\"5\"> Programming Skills </font>\n",
    "- Functions and Modules\n",
    "    \n",
    "<font size=\"5\"> Embedded Systems </font>\n",
    "- Thonny and Micropython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40ef17dd-1c68-43ed-9406-c2b55840c4b5",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange;\"> 0. Required Hardware </span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec4e4a28-936f-4676-8286-c69f06d2ac08",
   "metadata": {},
   "source": [
    "- Raspberry Pi Pico\n",
    "- Breadboard\n",
    "- USB connector\n",
    "- Camera (Arducam HM01B0) \n",
    "- 8 Wires"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60f85e09-35cc-47d9-b1d8-6e88b0fb6416",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h3 style=\"background-color:lightblue\"> Write your name and email below: </h3>\n",
    "\n",
    "**Name:** Shiva Sai Vummaji\n",
    "\n",
    "**Email:** svummaji@purdue.edu\n",
    "\n",
    "*You should work in groups of 2-3 for this lab.*\n",
    "<h3 style=\"background-color:lightblue\"> Write your groupmates' names below: </h3>\n",
    "\n",
    "**Groupmate(s):** Dhruv and Shawanwit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61de84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9b4a375-00e6-40d3-bea2-dcc8a1492083",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange;\"> 1. Background Subtraction </span>\n",
    "\n",
    "There is a popular image processing technique called background subtraction. The most effective way to utilize background subtraction for motion detection is when:\n",
    "\n",
    "1. the camera is stationary\n",
    "2. the background is static\n",
    "3. the lighting conditions do not change\n",
    "4. there is minimal noise in the image\n",
    "\n",
    "One scenario where all of these conditions are met is in production/manufacturing lines. In controlled indoor environments, long-term changes are rare.\n",
    "\n",
    "Essentially, background subtraction allows for the separation of the background from the foreground in an image. For instance, if a camera is set up to monitor a doorway, most of the time, nothing is in motion within the frame. By selecting an initial reference frame, we can compare all subsequent frames to the reference frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b45b9b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 14:53:59.410 python[98462:7028281] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-05 14:53:59.410 python[98462:7028281] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "vid = cv2.VideoCapture('test_vid.mov') \n",
    "height = vid.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "width  = vid.get(cv2.CAP_PROP_FRAME_WIDTH) \n",
    "scale = 0.125                                           # smaller scale for faster computations\n",
    "new_size = (int(width*scale),int(height*scale))\n",
    "frames = []                                             # saving frames to a list so that you can try\n",
    "                                                        # methods quickly without reloading the video\n",
    "while vid.isOpened():   \n",
    "    success, frame = vid.read()\n",
    "    if not success:\n",
    "        print(\"Unable to read frame. Exiting ...\")\n",
    "        break\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = cv2.resize(frame,dsize=new_size)\n",
    "    frames.append(frame)\n",
    "    cv2.imshow('frame', frame)                          # display grayscaled video resized, no other alterations\n",
    "    if cv2.waitKey(25) == ord('q'):                     # press Q on keyboard to stop\n",
    "        break\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5259bfd5",
   "metadata": {},
   "source": [
    "Background subtraction refers to a whole class of methods. However, for the scope of this lab, we will only be using one specific method. To demonstrate an example, we will utilize the same test video from the previous lab. To keep things concise, we won't delve into the detailed workings of `cv2.createBackgroundSubtractorMOG2()`. Generally, this method is slightly more robust against illumination changes. The output of the background subtraction function is known as a *mask*, which indicates that **the output is a binary image**. Pixel values in the mask are either 0 (representing the background) or 255 (representing the foreground).\n",
    "\n",
    "Run the cell to observe the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "097e595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgbg = cv2.createBackgroundSubtractorMOG2(detectShadows=False)      # define this before looping through the frames\n",
    "\n",
    "for f in frames:\n",
    "    img = fgbg.apply(f)\n",
    "    cv2.imshow('background subtracted frame',img)\n",
    "    if cv2.waitKey(25) == ord('q'):                                 # press Q on keyboard to stop\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "279f9240",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange;\"> 2. Motion Detection and Localization </span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7636de42-f083-40b1-8510-b5a0e6a57eeb",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Exercise 1</span>\n",
    "\n",
    "Suppose we apply background subtraction to each video frame and divide the frame into an $M\\times N$ grid, resulting in a total of $MN$ blocks. Can you consider a (mathematical) rule to determine whether there is movement in a specific block?\n",
    "\n",
    "*Hint: Would the proportion of certain grayscale values be high or low when there is motion?*\n",
    "\n",
    "<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 1 Below </h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29f00a49",
   "metadata": {},
   "source": [
    "Yes, we can consider a mathematical rule to determine the movement in a specific block. I can utilize the magnitudes of optical flow vectors (formula is by taking derivative of sqrt(u^2 + v^2) where u and v represent the optical flow vector using openCV). I can also utilize Fréchet Video Motion Distance formula, which ||μ_real - μ_gen||^2 + Tr(Σ_real + Σ_gen - 2(Σ_real * Σ_gen)^(1/2)). In this formula, μ_real and μ_gen refer to mean vectors of the motion features extracted from the real and generated videos. Σ_real and Σ_gen are covariance matrices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2adf1b0e",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange;\"> 3. Connecting the Camera </span>\n",
    "\n",
    "This time, we will record our own videos using the Arducam HM01B0, which is a small camera that can be connected to the Pico. \n",
    "\n",
    "### Wiring Instructions\n",
    "\n",
    "Please ensure that your microcontroller is not connected to the computer while you are wiring components together. If you are unsure about your wiring, please consult the instructor. Use your jumper wires to establish the following connections:\n",
    "\n",
    "| HM01B0 | Pico |\n",
    "|--------|------|\n",
    "| VCC    | 3V3  |\n",
    "| SCL    | GP5  |\n",
    "| SDA    | GP4  |\n",
    "| VSYNC  | GP16 |\n",
    "| HREF   | GP15 |\n",
    "| PCLK   | GP14 |\n",
    "| DO     | GP6  |\n",
    "| GND    | GND  |\n",
    "\n",
    "Here is an image of the completed breadboard:\n",
    "\n",
    "![img](https://raw.githubusercontent.com/TheDataScienceLabs/DSLab_Probability/main/book/labs/shared_files/connecting_the_camera/HM01B0_and_pico.png)\n",
    "\n",
    "<!-- ![img](camera.jpg)\n",
    "\n",
    "*Wiring the Arducam HM01B0 camera* \n",
    "\n",
    "![img](camerawiring.jpg)\n",
    "\n",
    "*Connections to the PICO on breadboard* -->\n",
    "\n",
    "To find the names of the pins on the Raspberry Pi Pico, you can refer to its pinout diagram located [here](https://datasheets.raspberrypi.com/pico/Pico-R3-A4-Pinout.pdf) or in the Extra Materials section. The HM01B0, on the other hand, should have its pins labeled.\n",
    "\n",
    "After confirming that the wiring is correct, press and hold the BOOTSEL button on the Pico while plugging it in. Download the [arducam.uf2](https://github.com/TheDataScienceLabs/DSLab_Probability/blob/main/book/labs/shared_files/connecting_the_camera/arducam.uf2) file and copy it onto the Pico's drive using your computer's file manager (it should be listed as an external drive: \"RPI-RP2\") and not with Thonny. Once the file transfer is complete, the Pico will automatically disconnect, and its LED will start blinking rapidly. \n",
    "\n",
    "Once the Pico has been successfully connected, please execute the following cell to ensure that we have successfully detected the Pico."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00c0d30f",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Exercise 2</span>\n",
    "\n",
    "Using your camera, locate the area of motion in the video frame in real time (or near real time). You can employ any method to accomplish this, whether it involves background subtraction, your solution to the previous exercise, a combination of the two, or any other approach you can think of. Determine how to visually indicate the detected movement on the camera, such as printing a statement in Python or drawing a rectangle on the frame.\n",
    "\n",
    "Display all the code you used in a cell below.\n",
    "\n",
    "*Note: If you encounter significant hardware issues, you can try applying your method to the test video instead.*\n",
    "\n",
    "<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 2 Below </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9baed999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are all the serial devices detected:\n",
      "/dev/cu.debug-console n/a\n",
      "/dev/cu.ShivasBoseFlexSoundLink n/a\n",
      "/dev/cu.Bluetooth-Incoming-Port n/a\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "No Raspberry Pi Pico was detected. Check to make sure it is plugged in, and that no other programs are accessing it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m port \u001b[38;5;129;01min\u001b[39;00m list_ports\u001b[38;5;241m.\u001b[39mcomports():\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(port\u001b[38;5;241m.\u001b[39mdevice, port\u001b[38;5;241m.\u001b[39mhwid)\n\u001b[0;32m---> 21\u001b[0m port \u001b[38;5;241m=\u001b[39m \u001b[43mget_pico_port\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mselected port \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as most likely to have a raspberry pi pico\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m, in \u001b[0;36mget_pico_port\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m pico_ports \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(list_ports\u001b[38;5;241m.\u001b[39mgrep(PICO_HWID))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pico_ports) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Raspberry Pi Pico was detected. Check to make sure it is plugged in, and that no other programs are accessing it\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pico_ports[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[0;31mException\u001b[0m: No Raspberry Pi Pico was detected. Check to make sure it is plugged in, and that no other programs are accessing it"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import serial\n",
    "from serial.tools import list_ports\n",
    "\n",
    "PICO_HWID = \"2E8A\"\n",
    "\n",
    "\n",
    "def get_pico_port():\n",
    "    pico_ports = list(list_ports.grep(PICO_HWID))\n",
    "    if len(pico_ports) == 0:\n",
    "        raise Exception(\n",
    "            \"No Raspberry Pi Pico was detected. Check to make sure it is plugged in, and that no other programs are accessing it\"\n",
    "        )\n",
    "    return pico_ports[0].device\n",
    "\n",
    "\n",
    "print(\"Here are all the serial devices detected:\")\n",
    "for port in list_ports.comports():\n",
    "    print(port.device, port.hwid)\n",
    "\n",
    "port = get_pico_port()\n",
    "print(f\"\\nselected port {port} as most likely to have a raspberry pi pico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2110ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = bytearray(96 * 96)\n",
    "img = np.zeros(shape=(96, 96), dtype=\"uint8\")\n",
    "\n",
    "with serial.Serial(port, timeout=1) as s:\n",
    "    s.read_until(b\"\\x55\\xAA\")\n",
    "    s.readinto(buffer)\n",
    "    img.flat[::-1] = buffer\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "render = ax.imshow(img, cmap='gray')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b00542",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with serial.Serial(port, timeout=1) as s:\n",
    "        while True:\n",
    "            s.read_until(b\"\\x55\\xAA\")\n",
    "            s.readinto(buffer)\n",
    "            img.flat[::-1] = buffer\n",
    "            frames_pico.append(img.copy())\n",
    "\n",
    "            img1 = fgbg.apply(frames_pico[-1])\n",
    "            img2 = cv2.Canny(frames_pico[-1], 200, 100)\n",
    "\n",
    "            img3 = img1 & img2\n",
    "\n",
    "            ret, thresh = cv2.threshold(img3, 127, 255, 0)\n",
    "            c, h = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            cv2.drawContours(img3, c, -1, (255, 255, 255), 3)\n",
    "            \n",
    "            render.set_data(img3)\n",
    "            fig.canvas.draw()\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57f9969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.11.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e10cca26",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Exercise 3</span>\n",
    "\n",
    "Provide a description of how effectively the motion detector/locator works in a paragraph or two. What methods did your group try? What challenges arose, and were you able to resolve any of them?\n",
    "\n",
    "<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 3 Below </h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58811bb8",
   "metadata": {},
   "source": [
    "The motion detector we implemented using OpenCV actually works very well, as it is able to detect hand movements, fingers, etc, very well. First, we applied the background subtraction and created img1. Then, we wanted to further improve the motion detection by adding an edge detection mechanism. We utilized the Canny() method that detects edges in an image. We generated img2 using edge detection. Then, we found the intersection of img1 and img2 and created img3, which basically implements both background subtraction and edge detection in order to effectively detect motion. However, there was an issue with Canny() because it created dots (circles) instead of actual lines, so it wasn't as visually apealing. As a result, we utilized the findCountours() and drawCountours() methods in order to draw contour lines from the dots/circles that we got for the edges from the Canny() method. The biggest challenge was to be able to research/find ways to improve the motion detector by implementing something else, in addition to the background subtraction. We resolved this by looking at ways to detect edges and utilizing Canny(). Another challenge was that we first did background subtraction and then did edge detection on the same image, so our result/output wasn't as good. In order to resolve this, we decided to apply both separately and then find the intersection/combine them in order to get better/more realistic output. Finally, we had another challenge because the findContours method changed, in terms of inputs and outputs, from opencv version 3.4 to 4.11, but we didn't at first understand the difference until we checked the version numbers. We resolved it by looking at an updated version of the documentation on the OpenCV website and realized that there is only 2 outputs (contours and hiearchy) and that we have to use the drawContours() methods in addition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc30300b",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> Reflection </span>\n",
    "\n",
    "Who would win in a fight? A billion lions or all the pokemon? Discuss with your group.\n",
    "All the pokemon because lions can't fly."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5733b5ebf5ecec2b002a59c36710d44decb4334b28aff8074b4cca610e6649ad"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
