{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff3f358",
   "metadata": {},
   "source": [
    "We will first implement the embed function. Remember that the input to the transformer is text. However, it does not directly deal with raw text. Instead, it represents everything as tensors. First, we have a tokenizer that splits a large chunk of text into arrays of tokens. For example, suppose our input is HelloWorld and our tokenizer decides that Hello is the first token and World is the second token. Our real input tensor is thus [0, 1]. We use a d_model dimension vector to represent one token. Suppose we have V tokens in total, this gives us a (V, d_model) matrix, where each row is a token vector. The embed function should look up relevant vectors for our input tokens. The input to embed is x with size (1, m) (technically, it's (m, ) in Numpy), the embed matrix W_E with size (V, d_model). The output should have size (m, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d9b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def embed(x, W_E):\n",
    "    return W_E[x]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30095141",
   "metadata": {},
   "source": [
    "Now that we have already implemented embed, we will implement positional embedding, which tells the Transformer about the position information. We will use the sinusoidal position embedding. You can refer to page 6 of the attention paper for the exact equations. The input m is the sequence length, d_model the model dimension, and the output should be a size (m, d_model) numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b90dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_embed(m, d_model):\n",
    "    pe = np.zeros((m, d_model))\n",
    "\n",
    "    pos = np.arange(m)[:, np.newaxis]\n",
    "    dim = np.arange(0, d_model, 2)[np.newaxis, :]\n",
    "\n",
    "    pe_temp = pos / (10000 ** (dim / d_model))\n",
    "\n",
    "    pe[:, 0::2] = np.sin(pe_temp)\n",
    "    pe[:, 1::2] = np.cos(pe_temp)\n",
    "\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "338035f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      " [ 8.41470985e-01  5.40302306e-01  9.98334166e-02  9.95004165e-01\n",
      "   9.99983333e-03  9.99950000e-01  9.99999833e-04  9.99999500e-01]\n",
      " [ 9.09297427e-01 -4.16146837e-01  1.98669331e-01  9.80066578e-01\n",
      "   1.99986667e-02  9.99800007e-01  1.99999867e-03  9.99998000e-01]\n",
      " [ 1.41120008e-01 -9.89992497e-01  2.95520207e-01  9.55336489e-01\n",
      "   2.99955002e-02  9.99550034e-01  2.99999550e-03  9.99995500e-01]\n",
      " [-7.56802495e-01 -6.53643621e-01  3.89418342e-01  9.21060994e-01\n",
      "   3.99893342e-02  9.99200107e-01  3.99998933e-03  9.99992000e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(pos_embed(5, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174b5ea",
   "metadata": {},
   "source": [
    "Implementation of RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829d6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsnorm(x, w=1, epsilon=1e-9):\n",
    "    term = np.mean(x*x, axis=1, keepdims = True) + epsilon\n",
    "    denominator = np.sqrt(term)\n",
    "    rms = (x / denominator) * w\n",
    "    return rms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b759c07",
   "metadata": {},
   "source": [
    "Next, we will implement softmax with temperature t, which is a key operation in attention. softmax normalizes a raw vector into a probability distribution. The input is of size (n_head, m, m) or (m, m) or (V, ), and the output has the same size. Your code should handle all cases robustly. The temperature t rescales logits before normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3839c0",
   "metadata": {},
   "source": [
    "Next, we will implement softmax with temperature t, which is a key operation in attention. softmax normalizes a raw vector into a probability distribution. The input is of size (n_head, m, m) or (m, m) or (V, ), and the output has the same size. Your code should handle all cases robustly. The temperature t rescales logits before normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb0883a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, t=1):\n",
    "    if t != 0:\n",
    "        x = x / t\n",
    "        x_new = x - np.max(x, axis = -1, keepdims = True)\n",
    "    \n",
    "        x_exp = np.exp(x_new)\n",
    "    \n",
    "        denominator = np.sum(x_exp, axis=-1, keepdims = True)\n",
    "    \n",
    "        return x_exp / denominator\n",
    "        \n",
    "    if t == 0:\n",
    "        x_maxs = np.max(x, axis=-1, keepdims = True)\n",
    "        mask = (x == x_maxs).astype(float)\n",
    "        counts = np.sum(mask, axis=-1, keepdims = True)\n",
    "        return mask/counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5f40a",
   "metadata": {},
   "source": [
    "Next, we will implement the apply_causal_mask function. The input x is of size (n_head, m, m). Remember in the lecture we discussed that the token i can only attend to the token j if \n",
    ". The input x[_][i][j] stores the raw score of how much token i attends to token j, without applying the causal mask. The output should be the same size as the input (still the the raw score), but with the causal mask applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3130d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_causal_mask(x, very_negative_number=-1e9):\n",
    "    n_head = x.shape[0]\n",
    "    m = x.shape[-1]\n",
    "    res = x.copy()\n",
    "\n",
    "    ones = np.ones((m, m), dtype = bool)\n",
    "\n",
    "    print(ones)\n",
    "\n",
    "    good = np.tril(ones)\n",
    "\n",
    "    print(good)\n",
    "\n",
    "    good = np.repeat(good[None, :, :], n_head, axis=0)\n",
    "\n",
    "    print(good)\n",
    "\n",
    "    res[~good] = very_negative_number\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5816424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True]\n",
      " [ True  True]]\n",
      "[[ True False]\n",
      " [ True  True]]\n",
      "[[[ True False]\n",
      "  [ True  True]]]\n",
      "[[[ 1.e+00 -1.e+09]\n",
      "  [ 1.e+00  1.e+00]]]\n"
     ]
    }
   ],
   "source": [
    "x = np.ones((1, 2, 2))\n",
    "\n",
    "res = apply_causal_mask(x)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a364dfa",
   "metadata": {},
   "source": [
    "You're going to implement the attention function in pure numpy! This is arguably the most complex part of this assignment. We will walk you through some details. The attention has n_head heads, and each head has d_head dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cae93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(x, W_Q, W_K, W_V, W_O):\n",
    "    \n",
    "    d_model, d_head, n_head = W_Q.shape\n",
    "    m = x.shape[0]\n",
    "\n",
    "    Q = np.einsum(\"md,dhn->mhn\", x, W_Q)\n",
    "    K = np.einsum(\"md,dhn->mhn\", x, W_K)\n",
    "    V = np.einsum(\"md,dhn->mhn\", x, W_V)\n",
    "\n",
    "    Q = Q.transpose(2, 0, 1)\n",
    "    K = K.transpose(2, 0, 1)\n",
    "    V = V.transpose(2, 0, 1)\n",
    "\n",
    "    scores = (Q @ K.transpose(0, 2, 1)) / np.sqrt(d_head)\n",
    "\n",
    "    masked_s = apply_causal_mask(scores)\n",
    "\n",
    "    attn_weights = softmax(masked_s)\n",
    "\n",
    "    context = attn_weights @ V\n",
    "\n",
    "    context = context.transpose(1, 0, 2).reshape(m, -1)\n",
    "\n",
    "    res = context @ W_O\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7be85f",
   "metadata": {},
   "source": [
    "Next, we will implement the gelu function using tanh approximation, which many state-of-the-art LLMs use as the activation function for the MLP. Please refer to page 2 of the original GELU paper for exact equations. The input x is of size (m, d_mlp), and the output is of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "778fa197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    res = 0.5*x*(1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715*x**3)))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b589fc",
   "metadata": {},
   "source": [
    "Next, we will implement the mlp layer. You are expected to use gelu that you just implemented. The MLP has two layers (i.e., one hidden layer and one output layer, with gelu applied only at the hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22ea06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, W_1, b_1, W_2, b_2):\n",
    "    hiddenL = x @ W_1 + b_1\n",
    "    hiddenL = gelu(hiddenL)\n",
    "\n",
    "    res = hiddenL @ W_2 + b_2\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d639e56",
   "metadata": {},
   "source": [
    "Congratulations on your achievements so far! You have all the mechanisms to implement a single transformer block. As before, our input is of size (m, d_model) and the output should have the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bb1389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "BlockParameter = namedtuple(\n",
    "    \"BlockParameter\",\n",
    "    [\n",
    "        \"W_1\",\n",
    "        \"b_1\",\n",
    "        \"W_2\",\n",
    "        \"b_2\",\n",
    "        \"W_Q\",\n",
    "        \"W_K\",\n",
    "        \"W_V\",\n",
    "        \"W_O\"\n",
    "    ]\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "151195b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(x, para: BlockParameter):\n",
    "    x_normalized = rmsnorm(x)\n",
    "\n",
    "    w_sum = attention(x_normalized, para.W_Q, para.W_K, para.W_V, para.W_O)\n",
    "\n",
    "    res_connection = x + w_sum\n",
    "\n",
    "    y_norm = rmsnorm(res_connection)\n",
    "\n",
    "    mlp_res = mlp(y_norm, para.W_1, para.b_1, para.W_2, para.b_2)\n",
    "\n",
    "    res_connection2 = res_connection + mlp_res\n",
    "\n",
    "    return res_connection2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bb3ef",
   "metadata": {},
   "source": [
    "The only piece we left before a toy but functional transformer model is the unembed layer. The unembed layer is a linear projection to the vocabulary space with a unnormalized probability distribution. Specifically, the input has size (m, d_model) and the output has size (m, V). W_U has the size (d_model, V)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "666ef166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unembed(x, W_U):\n",
    "    return np.einsum(\"md,dv->mv\", x, W_U)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be53f94",
   "metadata": {},
   "source": [
    "We now have all the mechanisms to implement a toy but functional transformer architecture! You are expected to use embed, pos_embed, block, and unembed. The input x is a list of token IDs and is of size (m, ), and the output is a unnormalized probability distribution over the next possible token, of size (m, V). We have n_block that chains to each other, and paras is a list of BlockParameters.\n",
    "Hints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fddd5f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def transformer(x, W_E, paras: List[BlockParameter], n_block):\n",
    "    \n",
    "    x_embed = embed(x, W_E)\n",
    "    m, d_model = x_embed.shape\n",
    "\n",
    "    pe = pos_embed(m, d_model)\n",
    "\n",
    "    embeds = x_embed + pe\n",
    "\n",
    "    for i in range(n_block):\n",
    "        embeds = block(embeds, paras[i])\n",
    "\n",
    "    logits = unembed(embeds, W_E.T)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4d71e",
   "metadata": {},
   "source": [
    "We will implement the self-supervised version of compute_loss. Suppose x is a list of tokens of length m (i.e., the input to the transformer function we have implemented). We already know that the transformer will output y of size (m, V): the unormalized probability of the next token. In other words, we can view the language modeling as m - 1 V-class classification problems, where the ground truth is x[i], and the predicted unormalized probability is y[i - 1], for \n",
    ". For each i, this is just a NLL loss. Your total loss should be the arithmetic mean of all the losses of m - 1 tokens. You should return a float number. All other inputs are the same as the transformer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b1a9702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(x, W_E, paras: List[BlockParameter], n_block):\n",
    "    y = transformer(x, W_E, paras, n_block)\n",
    "    loss = []\n",
    "\n",
    "    gt = x[1:]\n",
    "    logits = y[:-1, :]\n",
    "\n",
    "    probs = softmax(logits)\n",
    "\n",
    "    correct = probs[np.arange(len(gt)), gt]\n",
    "\n",
    "    loss = -np.log(correct)\n",
    "\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4e50d",
   "metadata": {},
   "source": [
    "Our final function is to implement topk_sample, which transforms a probability distribution into actual tokens. To make sampling high-quality and efficient, topk_sample will only consider the top k tokens, as the name suggests, where \n",
    ". In this function, you are given a prompt x of m tokens, go through the transformer function you implemented, use topk_sample to sample a new token, append that token to x, and go through this process again, until you sample n new tokens in total. You are expected to use transformer and softmax. All inputs are the same as in transformer or softmax. Note that the t here only refers to the temperature during sampling; you don't need to change the temperature inside the attention layer. You should return a numpy array of size (n, ) for new n token IDs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8236bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_sample(x, W_E, paras, n_block, n, k, t=1):\n",
    "    ### Don't change the following line for auto-grading reasons\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    res = []\n",
    "    curr_seq = x.copy()\n",
    "\n",
    "    for i in range(n):\n",
    "        logits_all = transformer(curr_seq, W_E, paras, n_block)\n",
    "        \n",
    "        last = logits_all[-1, :]\n",
    "\n",
    "        if t == 0:\n",
    "            token = np.argmax(last)\n",
    "\n",
    "        else:\n",
    "            last /= t \n",
    "\n",
    "        top_k = np.argsort(last)[-k:]\n",
    "\n",
    "        logits_top = last[top_k]\n",
    "\n",
    "        probs = softmax(logits_top)\n",
    "\n",
    "        sample_idx = rng.choice(k, p=probs)\n",
    "        token = top_k[sample_idx]\n",
    "\n",
    "        curr_seq = np.concatenate((curr_seq, [token]))\n",
    "        res.append(token)\n",
    "\n",
    "    return np.array(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ptm_env)",
   "language": "python",
   "name": "ptm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
